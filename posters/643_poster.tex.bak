%\documentclass[landscape,a0b,final,a4resizeable]{a0poster}
\documentclass[landscape,a0b,final]{a0poster}
%\documentclass[portrait,a0b,final,a4resizeable]{a0poster}
%\documentclass[portrait,a0b,final]{a0poster}
%%% Option "a4resizeable" makes it possible ot resize the
%   poster by the command: psresize -pa4 poster.ps poster-a4.ps
%   For final printing, please remove option "a4resizeable" !!

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{multicol}
\usepackage{multirow}
\usepackage[usenames,dvipsnames]{color}
\usepackage{psfrag}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{pstricks,pst-grad,calc}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage{tikz}
\usetikzlibrary{arrows,shapes,snakes,automata,backgrounds,fit,petri}

% Definitions
\setlength{\columnsep}{3cm}
\setlength{\columnseprule}{2mm}
\setlength{\parindent}{0.0cm}

% Background
\newcommand{\background}[3]{
	\newrgbcolor{cbegin}{#1}
	\newrgbcolor{cend}{#2}
	\psframe[fillstyle=gradient, gradlines=1000, gradend=cend, gradbegin=cbegin,
           	 gradangle=0, gradmidpoint=#3]( 0., 0. )( 1.\textwidth, -1.\textheight )
}

% Poster environment
\newenvironment{poster}{
	\begin{center} \begin{minipage}[c]{0.98\textwidth} }{
	\end{minipage} \end{center}
}

% Custom column
\newenvironment{pcolumn}[1]{
	\begin{minipage}{#1\textwidth} \begin{center} }{
  	\end{center} \end{minipage}
}

% Custom box
\newcommand{\pbox}[4]{
	\psshadowbox[#3]{
		\begin{minipage}[t][#2][t]{#1}
			#4
		\end{minipage}
}}

% Custom section
\newcommand{\csection}[1]{
\vspace{1.25cm}
\begin{center}
	\pbox{0.8\columnwidth}{}{linewidth=2mm, framearc=0.0, linecolor=lightgreen, fillstyle=gradient,
	                         gradangle=0, gradbegin=white, gradend=whitegreen, gradmidpoint=1.0, framesep=0.6em, shadowsize=0
	                        }{\begin{center}{\bf #1}\end{center}}
\end{center}
\vspace{1.25cm}
}

% Custom caption
\setcounter{figure}{1}
\setcounter{table}{1}

\newcommand{\tcaption}[1]{
  \vspace{0.3cm}
  \begin{quote}
    {{\sc Table} \arabic{table}: #1}
  \end{quote}
  %\vspace{0.3cm}
  \stepcounter{table}
}

\newcommand{\fcaption}[1]{
  \vspace{0.3cm}
  \begin{quote}
    {{\sc Figure} \arabic{figure}: #1}
  \end{quote}
  \vspace{0.6cm}
  \stepcounter{figure}
}

\renewcommand\refname{ }

% Math definitions
\input{./miscdefs.tex}

\begin{document}

\background{0.988 0.988 0.988}{0.933 0.933 0.933}{1.0}
\vspace*{1cm}

\newrgbcolor{lightblue}{0. 0. 0.80}
\newrgbcolor{white}{0.988 1.000 0.960}
\newrgbcolor{whiteblue}{.80 .80 1.}
\newrgbcolor{lightgreen}{0.349 0.376 0.431}
\newrgbcolor{whitegreen}{0.678 0.658 0.580}

\begin{poster}
%
\begin{center}
\begin{pcolumn}{0.99}
%
\pbox{0.95\textwidth}{}{linewidth=2mm, framearc=0.0, linecolor=lightgreen, fillstyle=gradient,
                        gradangle=0,gradbegin=white,gradend=whitegreen,gradmidpoint=1.0,framesep=0.5em,shadowsize=0}
{
% University logo
\begin{minipage}[c][7cm][c]{0.1\textwidth}
	\begin{center}
    	\includegraphics[width=12cm,angle=0]{images/dtu_logo}
	\end{center}
\end{minipage}
% Title and Authors
\begin{minipage}[c][7cm][c]{0.78\textwidth}
 	\begin{center}
    	{\huge {\bf Bayesian Sparse Factor Models and DAGs Inference and Comparison} } \\ [10mm]
    	{\Large Ricardo Henao and Ole Winther} \\ [7.5mm]
     	\begin{tabular}{cc}
			DTU Compute $\cdot$ Technical University of Denmark \hspace{1cm} & \hspace{1cm} Bioinformatics Centre $\cdot$ University of Copenhagen \\
			Kgs. Lyngby, Denmark & Copenhagen N, Denmark
		\end{tabular}
  	\end{center}
\end{minipage}
% Department logo
\begin{minipage}[c][7cm][c]{0.1\textwidth}
	\begin{center}
   		\includegraphics[width=12cm,angle=0]{images/imm_logo}
	\end{center}
\end{minipage}
%
}
\end{pcolumn}
\end{center}
%
\vspace*{0.5cm}
%
\begin{multicols}{3}
%
\csection{Introduction}
%
\begin{itemize}
	\item We present a novel approach to learn directed acyclic graphs (DAGs) and factor models within the same framework while also allowing for model comparison between them.
	\item We exploit the connection between factor models and DAGs to propose Bayesian hierarchies based on spike and slab priors to promote sparsity, heavy-tailed priors to ensure identifiability and predictive densities to perform the model comparison.
\end{itemize}
%
\csection{Model Specification}
%
%\vspace{1cm}
{\bf From DAGs to Factor Models}
\vspace{0.6cm}

We assume that an ordered $d$-dimensional data vector $\P\x$ can be represented as a DAG with only observed nodes, where $\P$ is an unknown permutation matrix, thus %and that the model is linear in such that the value of each variable is a linear weight combination of parent nodes plus a driving signal $\z$:
%
\begin{equation}  \label{eq:Az}
	\x=\P^{-1} \B\P\x+\z \ , \ \textrm{(DAG model)}
\end{equation}
%
where $\B$ is a strictly lower triangular square matrix and $\z$ is a driving signal. In this setting, each non-zero element of $\B$ corresponds to a link in the DAG. Solving for $\B$ we can rewrite
%
\begin{equation} \label{eq:Bz}
	\x=\P\inv\A\P\z=\P\inv(\I-\B)\inv\P\z \ , \ \textrm{(Noiseless factor model)}
\end{equation}
%
\begin{itemize}
	\item $\P\inv\A\P$ must be sparse so it can be permuted to lower triangular since $(\I-\B)\inv$ is triangular.
	\item $\z$ must be non-Gaussian to ensure identifiability \cite{comon94}.
	\item $\P$ is unknown, we can estimate $\P\inv\A\P$ and then stochastically find $\P$.
\end{itemize}

\vspace{0.6cm}
{\bf From Factor Models to DAGs}
\vspace{0.6cm}

Instead of using the noise-free factor model of equation \eqref{eq:Bz} we allow for additive noise
%
\begin{equation*} \label{eq:Aze}
	\x=\PL\inv\A\PR\z+\bepsilon \ , \ \textrm{(Factor model)}
\end{equation*}
%
where $\bepsilon$ is uncorrelated Gaussian noise, $\PL=\P$ is the permutation matrix for the rows of $\A$ and $\PR=\PC\PL$ another permutation for the columns of $\A$ with $\PC$ accounting for the permutation freedom of the factors. The Bayesian model is specified as follows
%
\begin{align*} \label{eq:Hos}
	\begin{aligned}
		\X|\PL,\A,\PR,\Z,\bPsi \ \sim & \ \DN(\X|\PL^{-1}\A\PR\Z,\bPsi) \ , & \quad \Z \ \sim & \ \pi(\Z|\cdot) \ , \ \textrm{(Heavy-tailed factor prior)} \\
		\psi_i\inv|s_s,s_r \ \sim & \ \Ga(\psi_i\inv|s_s,s_r) \ , & \A \ \sim & \ \rho(\A|\cdot) \ , \ \textrm{(Sparse mixing prior)}
	\end{aligned}
\end{align*}
%
{\bf Identifiability:} We are restricted to use non-Gaussian distributions $\pi(\Z|\cdot)$ for the factors $\z_n$, here we use Laplace distributions parameterized as scale mixtures of Gaussians \cite{andrews74}
%
\begin{align*}
z_{jn}|\mu,\lambda \ \sim & \ \La(z_{jn}|\mu,\lambda) = \int_0^\infty\DN(z_{jn}|\mu,\upsilon)\Exp(\upsilon_{jn}|\lambda^2)d\upsilon_{jn} \ , \\
\lambda^2|\ell_s,\ell_r \ \sim & \ \Ga(\lambda^2|\ell_s,\ell_r) \ ,
\end{align*}

\vspace{0.6cm}
{\bf Sparsity:} We require a sparse prior $\rho(\A|\cdot)$ able to produce exact zeros in $\A$. Here we adopt a two-layer discrete {\color{BrickRed} spike} and {\color{ForestGreen} slab} prior for the elements $a_{ij}$ of $\A$ similar to the one in \cite{lucas06}
%
\begin{minipage}[b]{0.46\linewidth}
\begin{align*}
	\begin{aligned}
		a_{ij}|r_{ij},\psi_i,\tau_{ij} \ \sim & \ (1-r_{ij}){\color{BrickRed}\delta(a_{ij})} + r_{ij}{\color{ForestGreen}\DN(a_{ij}|0,\psi_i\tau_{ij})} \ , \\
		{\color{gray} r_{ij}|\eta_{ij} \ \sim} & \ {\color{gray} \Ber(r_{ij}|\eta_{ij}) \ ,} \\
		\eta_{ij}|q_{ij},\alpha_p,\alpha_m \ \sim & \ (1-q_{ij}){\color{BrickRed}\delta(\eta_{ij})} \\
		& \hspace{10mm} + q_{ij}{\color{ForestGreen}\Be(\eta_{ij}|\alpha_p\alpha_m,\alpha_p(1-\alpha_m))} \ , \\
		{\color{gray} q_{ij}|\nu_j \ \sim} & \ {\color{gray} \Ber(q_{ij}|\nu_j) \ ,} \\
		{\color{gray} \tau_{ij}\inv|t_s,t_r \ \sim} & \ {\color{gray} \Ga(\tau_{ij}\inv|t_s,t_r) \ ,} \\
		\nu_j|\beta_m,\beta_p \ \sim & \ \Be(\nu_j|\beta_p\beta_m,\beta_p(1-\beta_m)) \ .
	\end{aligned}
\end{align*}
\end{minipage}
\begin{minipage}[b]{0.41\linewidth}
	\centering
			\begin{tikzpicture}[bend angle=45,>=latex,font=\small]

			  \tikzstyle{obs} = [ circle, thick, draw = black!100, fill = blue!20, minimum size = 3mm ]
			  \tikzstyle{lat} = [ circle, thick, draw = black!100, fill = red!0, minimum size = 3mm ]
			  \tikzstyle{par} = [ circle, draw, fill = black!100, minimum width = 1pt, inner sep = 0pt]	

			  \tikzstyle{every label} = [black!100]

			  \begin{scope}[node distance = 2.12cm and 2.12cm,rounded corners=4pt]
				\node [obs] (x)  [ label = -90:$x_{in}$ ] {};
				\node [lat] (z) [ above left of = x, node distance = 3cm, label = -90:$z_{jn}$] {}
					edge [post] (x);
				\node [lat] (upsilon) [ above of = z, label = 90:$\upsilon_{jn}$] {}
					edge [post] (z);
				\node [lat] (lambda) [ left of = upsilon, label = 90:$\lambda$] {}
					edge [post] (upsilon);
				\node [lat] (A) [ above right of = x, node distance = 3cm, label = 45:$a_{ij}$] {}
					edge [post] (x);
				\node [lat] (phi) [ below of = A, label = 0:$\psi_i$] {}
					edge [post] (x)
					edge [post] (A);
				\node [lat] (R) [ right of = A, label = -90:$r_{ij}$] {}
					edge [post] (A);
				\node [lat] (eta) [ right of = R, label = -90:$\eta_{ij}$] {}
						edge [post] (R);
				\node [lat] (Q) [ above of = eta, label = 180:$q_{ij}$] {}
						edge [post] (eta);
				\node [lat] (nu) [ above of = Q, label = 180:$\nu_j$] {}
						edge [post] (Q);
				\node [lat] (tau) [ above of = A, label = 90:$\tau_{ij}$] {}
						edge [post] (A);
				\draw (-2.4,-1.1) node {\tiny{$n=1:N$}};
				\draw (-2.3,6.6) node {\tiny{$j=1:d$}};
				\draw (6.3,-0.9) node {\tiny{$i=1:d$}};

				\begin{pgfonlayer}{background}
	                \filldraw [line width = 1pt, draw=black!50, fill=black!5]
						(0.9cm,5.9cm)  rectangle (-3.5cm,-1.5cm)
	                    (7.3cm,5.7cm)  rectangle (-0.9cm,-1.3cm)
						(7.1cm,7.0cm)  rectangle (-3.3cm,0.7cm);
				\end{pgfonlayer}
			  \end{scope}
			\end{tikzpicture}\vspace{-0.25cm}
	%\fcaption{Graphical model for Bayesian hierarchy in equation \eqref{eq:Hos}.} \label{fg:osgm}
\end{minipage}
\vspace{0.6cm}

We make the following Bayesian specification of linear DAG model of equation \eqref{eq:Az} as
%
\begin{align*}
	\X|\PL,\B,\X,\cdot \ \sim \ \pi(\X-\PL^{-1}\B|\cdot) \ , \quad \B \ \sim \ \rho(\B|\cdot) \ , \ \textrm{(DAG model)}
\end{align*}
%
where $\pi(\cdot)$ and $\rho(\cdot)$ are given above, $\B$ is a strictly lower triangular matrix and we use $\lambda_1,\ldots,\lambda_d$ to compensate for the fixed scaling of $\X$.

\vspace{0.6cm}
{\bf Permutation Search, $\PL$ and $\PR$:} We perform a stochastic search over the space of all possible $d!$ orderings in the form of a Metropolis-Hastings (MH) algorithm.
%
\begin{itemize}
	\item Acceptance probability $\min(1,\xi_{\rightarrow\star})$ where $\xi_{\rightarrow\star}=\frac{\DN(\X|(\PL^\star)^{-1}(\M\odot \PL^\star \A (\PR^\star)\inv)\PR^\star,\bPsi)}{\DN(\X|\PL^{-1}(\M\odot \PL\A\PR\inv)\PR,\bPsi)}$.
	\item Symmetric proposal consisting on a single uniform random transposition of $\PL$ and $\PR$.
	\item $\M$ is lower triangular and binary, to break the invariability of the model to permutations.
\end{itemize}
%
\vspace{0.6cm}
{\bf Predictive distributions:} we use $p(\X^\star|\X,\MOD)$ with $\MOD=\{\MOD_\FA,\MOD_\DAG\}$ instead of marginal likelihoods. With Gibbs sampling, we draw samples from $p(\A,\bPsi,\lambda|\X,\cdot)$ and $p(\B,\lambda_1,\ldots,\lambda_m|\X,\cdot)$. Then we average over $p(\Z^\star|\cdot)$ for a test set $\Z^\star$ using (permutation matrices are omitted for clarity)
%
\begin{align*}
	p(\X^\star|\A,\bPsi,\cdot) & = \int p(\X^\star|\A,\Z,\bPsi)p(\Z|\cdot)d\Z \approx \frac{1}{\rep}\prod_{n} \sum_{r}^\rep \DN(\x_n^\star|\0,\A\ts\U_n\A+\bPsi) \ , \ \textrm{(factor model)} \\
	p(\X^\star|\B,\cdot) & = \int p(\X^\star|\B,\X,\Z)p(\Z|\cdot)d\Z = \prod_{i,n} \La(x_{ij}|[\B\X]_{in},\lambda_i) \ , \ \textrm{(DAG)}
\end{align*}
%
where $\U_n=\diag(\upsilon_{1n},\ldots,\upsilon_{dn})$, the $\upsilon_{jn}$ are sampled from the prior and $[\B\X]_{ij}$ is element of $\B\X$.
%
% \vspace{0.6cm}
% {\bf Algorithm:} We can use the factor model in \eqref{eq:Hos} jointly MH updates to produce a set of orderings ($m_\topc=6$), then we perform inference on the DAG model in \eqref{eq:Hss} and select the best one.
%
% \vspace{1cm}
% \begin{center}
% \begin{minipage}[c][8cm][c]{0.28\textwidth}
% 	\hline
% 	\vspace{0.2cm}
% 	\begin{algorithmic}[1]
% 	\REQUIRE Data ${\bf X}$, sampler parameters and number of candidates ($m_\topc$)
% 	\ENSURE $\B$ and $\PL$	
% 	\STATE Standardize $\X$
% 	\STATE {\bf Order search} Run sampler on $\X$ to get $\PL^{(i)}$ for $i=1,\ldots,m_\topc \quad$ (eq. \ref{eq:Hos})
% 	\WHILE{$i\leq m_\topc$}
% 	\STATE {\bf Structure search} Run sampler on $\X$ and $\PL^{(i)}$ to get $\B^{(i)}$ and $\mathcal{L}^{(i)} \quad$ (eq. \ref{eq:Hss})
% 	\ENDWHILE
% 	\STATE Select the model, $i^\star = \argmax \widetilde{\lik}^{(i)}$, then $\B\leftarrow\B^{(i^\star)}$ and $\PL\leftarrow\PL^{(i^\star)}$
% 	%\STATE $\B\leftarrow\B^{(i^\star)}$ and $\PL\leftarrow\PL^{(i^\star)}$
% 	\end{algorithmic}
% 	\vspace{0.2cm}
% 	\hline
% \end{minipage}	
% \end{center}
%
%
\csection{Experiments}
%
%We consider four sets of experiments in the following. The first two consist on extensive experiments using artificial data, the third addresses the model comparison scenario and the last one uses real data previously published in \cite{sachs05}.
%
%\vspace{0.8cm}
{\bf LiNGAM suite}
\vspace{0.8cm}

%
\begin{itemize}
	\item We compare against LiNGAM using the artificial model generator presented with LiNGAM \cite{shimizu06}.
	\item Both dense and sparse non-Gaussian networks with different degree of sparsity.
	\item The variables are randomly permuted to hide the correct order, $\P$.
	\item We consider $d=\{5,10\}$ and $N=\{200,500,1000,2000\}$.
	% \item Fixed hyperparameters for all cases.
	%\item Comparison against LiNGAM \cite{shimizu06}.
\end{itemize}
%
\vspace{0.5cm}
\begin{center}
		\begin{psfrags}
		\psfrag{tpr}[c][c][0.6]{True positive rate}\psfrag{tnr}[c][c][0.6]{True negative rate}\psfrag{a1r}[c][c][0.6]{AUC}\psfrag{oer}[c][c][0.6]{Orderings error rate}\psfrag{ns}[c][c][0.6]{$N$}\psfrag{d=5 Ours}[l][l][0.45]{$d=5$ SLIM}\psfrag{d=5 LINGAM}[l][l][0.45]{$d=5$ LiNGAM}\psfrag{d=10 Ours}[l][l][0.45]{$d=10$ SLIM}\psfrag{d=10 LINGAM}[l][l][0.45]{$d=10$ LiNGAM}
		\begin{tabular}{cccc}
			% \hline
			\includegraphics[scale = 0.9, viewport = 10 30 240 450, clip]{./images/lingam_tpr.eps} &
			\includegraphics[scale = 0.9, viewport = 10 30 240 450, clip]{./images/lingam_tnr.eps} &
			\includegraphics[scale = 0.9, viewport = 10 30 240 450, clip]{./images/lingam_auc.eps} &
			\includegraphics[scale = 0.9, viewport = 10 30 240 450, clip]{./images/lingam_oe.eps} \\
			% \hline
			{\small(a)} & {\small(b)} & {\small(c)} & {\small(d)} %\vspace{-0.5cm}
		\end{tabular}
    \end{psfrags}
\fcaption{Performance measures for LiNGAM suite. (a) True positive rate. (b) True negative rate. (c) Frequency of AUC being greater than 0.9. (d) Number of estimated correct orderings.}
\end{center}
\vspace{-0.5cm}
%
\vspace{0.8cm}
{\bf Bayesian networks repository}
\vspace{0.8cm}
%
\begin{itemize}
	\item 7 structures: alarm ($d=37$), barley (48), carpo (61), hailfinder (56), insurance (27), mildew (35) and water (32).
	\item A single dataset of size $N=1000$ is generated from each network.
	% \item Fixed hyperparameters for all cases.
	\item Comparison against: L1MB then DAG-search (DSL) \cite{schmidt07}.
\end{itemize}
%
\vspace{0.5cm}
\begin{center}
		\begin{psfrags}
			\psfrag{fp}[c][c][0.6]{False positive rate}\psfrag{auc}[c][c][0.6]{AUC}\psfrag{rev}[c][c][0.6]{Reversed links}\psfrag{fn}[c][c][0.6]{False negative rate}\psfrag{water}[c][c][0.5]{water}\psfrag{mildew}[c][c][0.5]{mildew}\psfrag{insurance}[c][c][0.5]{insurance}\psfrag{hailfinder}[c][c][0.5]{hailfinder}\psfrag{carpo}[c][c][0.5]{carpo}\psfrag{barley}[c][c][0.5]{barley}\psfrag{alarm}[c][c][0.5]{alarm}\psfrag{DS}[c][c][0.5]{DS}\psfrag{OS}[c][c][0.5]{OS}\psfrag{OSC}[c][c][0.5]{OSC}\psfrag{DSC}[c][c][0.5]{DSC}\psfrag{DSL}[c][c][0.5]{DSL}\psfrag{sFA}[c][c][0.5]{Ours}
		\begin{tabular}{ccc}
			% \hline
			\includegraphics[scale = 0.98,viewport = 20 0 240 300, clip ]{./images/bnrepo_tpr.eps} &
			\includegraphics[scale = 0.98,viewport = 20 0 240 300, clip]{./images/bnrepo_tnr.eps} &
			\includegraphics[scale = 0.98,viewport = 20 0 240 300, clip]{./images/bnrepo_auc.eps} \\
			% \hline
			{\small(a)} & {\small(b)} & {\small(c)} %\vspace{-0.5cm}
		\end{tabular}
    \end{psfrags}
\fcaption{Performance measures for Bayesian networks repository experiments.}
\end{center}
\vspace{-0.5cm}
%
\vspace{0.8cm}
{\bf Model comparison}
\vspace{0.8cm}
%
\begin{itemize}
	\item 1000 different datasets with $d=5$ and $N=\{500,1000\}$.
	\item Approximately half of the datasets were generated using DAGs.
	\item We kept $20\%$ of the data to compute the predictive densities to then select between DAGs and factor models.
	% \item Fixed hyperparameters for both cases.
\end{itemize}
%
\begin{center}
	\begin{tabular}{cccc}
		\hline
		$N$ & True DAG & True factor model & Error \\
		\hline
		500 & $91.5\%$ & $89.2\%$ & $9.6\%$ \\
		1000 & $98.5\%$ & $94.6\%$ & $5.0\%$ \\
		\hline
	\end{tabular}
	\tcaption{Model selection accuracies and overall error rates.}
\end{center}
%
\vspace{0.8cm}
{\bf Protein-signaling network}
\vspace{0.8cm}

The dataset introduced by \cite{sachs05} consists on flow cytometry measurements of 11 different proteins.
%
\begin{itemize}
	\item Observations are vectors of quantitative amounts measured from single cells.
	\item Data generated from a series of stimulatory cues and inhibitory interventions.
	\item Observational data only, 1755 observations corresponding to general stimulatory conditions.
	\item Our method found 10 true links (TP), one falsely added link (FP).
	\item Our method found two reversed links (RL). PIP$_2\rightarrow$ PIP$_3$ is bidirectional and PLC$\gamma\rightarrow$ PIP$_3$ was also found reversed in \cite{sachs05} using interventional data.
	\item We also tried the methods above. Results were: TP$\approx9$, TN$\approx32$ and RL$\geq 6$.
\end{itemize}
%
\vspace{0.5cm}
\begin{center}
	\begin{psfrags}
		 \psfrag{raf}[c][c][0.6][0]{Raf}\psfrag{erk}[c][c][0.6][0]{Erk}\psfrag{p38}[c][c][0.6][0]{p38}\psfrag{jnk}[c][c][0.6][0]{Jnk}\psfrag{akt}[c][c][0.6][0]{Akt}\psfrag{mek}[c][c][0.6][0]{Mek}\psfrag{pka}[c][c][0.6][0]{PKA}\psfrag{pkc}[c][c][0.6][0]{PKC}\psfrag{pip2}[c][c][0.6][0]{PIP$_2$}\psfrag{pip3}[c][c][0.6][0]{PIP$_3$}\psfrag{plcy}[c][c][0.6][0]{PLC$\gamma$}\psfrag{1.00}[c][c][1][0]{}
		\psfrag{freq}[b][c][0.6][0]{Usage $\%$}\psfrag{cand}[t][c][0.6][0]{Orderings}\psfrag{lik}[t][c][0.6][0]{Log-likelihood}\psfrag{den}[b][c][0.6][0]{Density}\psfrag{lr}[b][l][0.6][0]{Ratio}\psfrag{acc}[t][l][0.6][0]{Accuracy}\psfrag{xx}[t][l][0.6][0]{Magnitude}\psfrag{fac}[t][l][0.6][0]{Factors}\psfrag{var}[t][l][0.6][0]{ }\psfrag{yy}[t][l][0.6][0]{ }\psfrag{Candidates}[c][c][0.3][0]{Candidates}
		\begin{tabular}{ccc}
			\includegraphics[scale=0.8]{./images/sachs_true.ps} &
			\includegraphics[scale=0.8]{./images/sachs_sachs.ps} &
			\includegraphics[scale=0.76]{./images/sachs_sFA.ps} \\
			{\small(a)} & {\small(b)} & {\small(c)} %\vspace{-0.5cm}
		\end{tabular}
		\begin{minipage}[c]{0.41\linewidth}
			\begin{tabular}{cc}
				\includegraphics[scale=0.6,viewport= 20 20 340 530, clip]{./images/sachs_liks.eps} &
				\includegraphics[scale=0.6,viewport= 20 20 310 530, clip]{./images/sachs_lrs.eps} \\
				{\small(d)} & {\small(e)}
			\end{tabular}
		\end{minipage}
		\begin{minipage}[c]{0.59\linewidth}
			\fcaption{Result for protein-signaling network. \\ (a) Textbook signaling network as reported in \cite{sachs05}. \\ (b) Estimated structure using Bayesian networks \cite{sachs05}. (c) Estimated structure using our model. \\ (d) Test likelihoods for the best ordering DAG (dashed) and the factor model (solid). \\ (e) Likelihood ratios (solid) and structure errors (dashed) for all candidates considered by our method and their usage.} \label{fg:sachs}
		\end{minipage}
	\end{psfrags}
\end{center}
\vspace{-0.5cm}
%
\csection{Conclusions \& Outlook}
%
\begin{itemize}
	\item Novel approach to perform inference and model comparison of sparse factor models and DAGs within the same framework.
	\item First time that a method for comparing such a closely related linear models is proposed.
	\item Results on artificial and real data showed that our method significantly outperforms state-of-the-art techniques for structure learning.
	\item Currently investigating extensions to other source distributions (non-parametric Dirichlet process, temporal Gaussian processes and discrete).
\end{itemize}
%
\csection{References}
%
\vspace{-2.5cm}
\bibliographystyle{is-unsrt}
%\footnotesize{
\bibliography{./mlbib}
%}
%
\end{multicols}
%
\end{poster}
%
\end{document}
