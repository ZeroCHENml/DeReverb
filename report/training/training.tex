 The time and frequency domain models are each trained for 30 epochs using the Adam optimizer \todo[inline]{cite this}. Training is stopped at this point because losses have largely stopped decreasing. Both models show converging trends, however it is important to note that because of the random transforms used in the data pipeline, large changes in loss can occur between epochs, and thus losses are not strictly decreasing for the training or validation set. For the time-domain model, a simple \ac{MSE} loss is used. It should be noted that while this loss is frequently used, it has several theoretical shortcomings when used in this context. The first of these is that a white signal (equal power at each frequency) will have larger amplitudes at lower frequencies. Errors at low frequencies will then take higher priority than errors at high frequencies. The second shortcoming of \ac{MSE} loss is that it is not scale invariant. Relative losses in quiet frames will be optimized far less than relative losses in loud frames. One final shortcoming is that \ac{MSE} in the time domain is highly sensitive to phase, unlike the human ear. For the frequency-domain model, a naive \ac{MSE} loss is also tried, in addition to a custom loss function. In the frequency domain when working with only amplitude, \ac{MSE} makes more sense than in the time domain, because it is phase-invariant. The other two shortcomings still apply, however. In order to counteract the over-weighting of low-frequency content, a simple custom technique is developed.  

The idea behind this custom loss function is to multiply each frequency bin of the model output and target by the perceptual relevance of that frequency before computing the loss function. This technique allows the squared term of \ac{MSE} loss to penalize errors in the most perceptually relevant frequencies.
