In real world environments, speech is heard as a mix of direct signal with reflections caused by the surrounding environment. Late reflections decrease speech intelligibility, causing problems for listeners, as well as tasks such as \ac{ASR}. Traditionally, the best approach for speech dereverberation is to use a multi-channel approach. The basic principle of these approaches is that direct path sound will have high coherence between multiple microphones, while reverberant sound will have low coherence \cite{buchholz}. Based on coherence, a gain function can be applied in the time-frequency domain to remove reverberant sound. These approaches have the downside that they require extra hardware. Additionally, traditional coherence based approaches do not take advantage of the structure of speech, only exploiting spatial differences between sounds. A method is then desired which can remove reverberation from a recording using only a single microphone which can exploit the structure of speech. The first of these problems has been approached using traditional signal processing methods to estimate the impulse response of the reverberation and perform de-convolution to obtain the clean signal. The problem with these approaches is that noise is often mixed with the signal, and this is often amplified greatly by small errors in the impulse response estimation. Based on their success when used in problems such as \ac{ASR}, Neural networks are known to be capable of learning the characteristics of speech. This should allow a neural network to remove reverberation that is mixed with non-stationary noise.
